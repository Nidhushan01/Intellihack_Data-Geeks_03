{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93c64deaccbe4386a67dcacafb20bd29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_791ee22b8499434b953bb958ac4c989f",
              "IPY_MODEL_db44f5e74eee4d47898abe35cc5eb3bb",
              "IPY_MODEL_70f41ef64c6a49afa3c7ead6269cd7b4"
            ],
            "layout": "IPY_MODEL_387e189b86b1407196c6a128e041c568"
          }
        },
        "791ee22b8499434b953bb958ac4c989f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8851c2da2c7d415db097343985ced603",
            "placeholder": "​",
            "style": "IPY_MODEL_6d47044f8d9f46dca3c2759588eca06e",
            "value": "Loading checkpoint shards:  50%"
          }
        },
        "db44f5e74eee4d47898abe35cc5eb3bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_921cf196b7d6489cbed9447ee1afd773",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5cbe58ffbfc945e2bae11cd3df9354d8",
            "value": 1
          }
        },
        "70f41ef64c6a49afa3c7ead6269cd7b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fa6f7d1e1724d45bcdf5eb8cf77e51c",
            "placeholder": "​",
            "style": "IPY_MODEL_7f652f862e2c481e80c6b38a97ab17b7",
            "value": " 1/2 [00:19&lt;00:19, 19.48s/it]"
          }
        },
        "387e189b86b1407196c6a128e041c568": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8851c2da2c7d415db097343985ced603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d47044f8d9f46dca3c2759588eca06e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "921cf196b7d6489cbed9447ee1afd773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cbe58ffbfc945e2bae11cd3df9354d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fa6f7d1e1724d45bcdf5eb8cf77e51c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f652f862e2c481e80c6b38a97ab17b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddbe7fbbf7f74175bb9ee919b037232c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_338850c32a0a4967be7ad7955055095f",
              "IPY_MODEL_5ba3b5355ad84e79b1a48fcc882183d7",
              "IPY_MODEL_fa2e6d629c884ec4ba29ef251b7720ef"
            ],
            "layout": "IPY_MODEL_bd570277e39945839e94f3f590c0c230"
          }
        },
        "338850c32a0a4967be7ad7955055095f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_906877a68b5c44b4b115c23e576710d7",
            "placeholder": "​",
            "style": "IPY_MODEL_91a06d92f9f948b39cd8973c04360bf1",
            "value": "Loading checkpoint shards:  50%"
          }
        },
        "5ba3b5355ad84e79b1a48fcc882183d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72906650a4804ad495ac017aa3c1c131",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7917f1373ad14a91b2766123faf7dcf1",
            "value": 1
          }
        },
        "fa2e6d629c884ec4ba29ef251b7720ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c4e7c2b0a7a41ffbd121cd13c0d8f00",
            "placeholder": "​",
            "style": "IPY_MODEL_1f9adb636f9b423d929561f44605fe30",
            "value": " 1/2 [00:17&lt;00:17, 17.51s/it]"
          }
        },
        "bd570277e39945839e94f3f590c0c230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "906877a68b5c44b4b115c23e576710d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91a06d92f9f948b39cd8973c04360bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72906650a4804ad495ac017aa3c1c131": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7917f1373ad14a91b2766123faf7dcf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c4e7c2b0a7a41ffbd121cd13c0d8f00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f9adb636f9b423d929561f44605fe30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets accelerate peft bitsandbytes unsloth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRS9chs-lakC",
        "outputId": "cd06eb78-8b5a-465e-e750-e2b26b02a629"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.0)\n",
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.3.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.3.8 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2025.3.8)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.28.post3)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.1.0)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.9.16)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.12.1)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.20.1+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.8->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.8->unsloth) (10.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.18.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T3jzgl6Tjb6g"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file and extraction folder\n",
        "zip_file_path = \"/content/q3_dataset-20250308T135953Z-001.zip\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "extract_folder = '/content/q3_dataset/'\n",
        "\n",
        "# Unzipping the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_folder = \"/content/q3_dataset/q3_dataset\"\n",
        "\n",
        "\n",
        "\n",
        "# List the files in the extracted folder\n",
        "os.listdir(extracted_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chsY_stVj_kD",
        "outputId": "bb53e549-81a8-40e7-f6fb-a5828024b3c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['open-source-week.md',\n",
              " 'deepseekv3-explained.md',\n",
              " 'deepseekv3-cost-explained.md',\n",
              " 'design-notes-3fs.md',\n",
              " '2501.12948v1.pdf',\n",
              " 'dataset.md']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading Markdown files\n",
        "md_folder = '/content/q3_dataset/q3_dataset'\n",
        "\n",
        "# List all markdown files\n",
        "md_files = [file for file in os.listdir(md_folder) if file.endswith('.md')]\n",
        "\n",
        "# Read and print the content of each markdown file\n",
        "md_text = []\n",
        "for md_file in md_files:\n",
        "    with open(os.path.join(md_folder, md_file), 'r') as f:\n",
        "        md_text.append(f.read())\n",
        "\n",
        "# Print a preview of the markdown contents\n",
        "print(md_text[:2])  # Print the first 2 markdown files' content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-5dCUbjkK-C",
        "outputId": "31a92857-f842-45a4-a465-25ceb84898a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"# 202502 Open-Source Week\\n\\nWe're a tiny team @deepseek-ai pushing our limits in AGI exploration.\\n\\nStarting this week , Feb 24, 2025 we'll open-source 5 repos – one daily drop – not because we've made grand claims, but simply as developers sharing our small-but-sincere progress with full transparency.\\n\\nThese are humble building blocks of our online service: documented, deployed and battle-tested in production. No vaporware, just sincere code that moved our tiny yet ambitious dream forward.\\n\\nWhy? Because every line shared becomes collective momentum that accelerates the journey. Daily unlocks begin soon. No ivory towers - just pure garage-energy and community-driven innovation 🔧\\n\\nStay tuned – let's geek out in the open together.\\n\\n## Day 1 - FlashMLA\\n\\nEfficient MLA Decoding Kernel for Hopper GPUs\\nOptimized for variable-length sequences, battle-tested in production\\n\\n🔗 FlashMLA GitHub Repo\\n✅ BF16 support\\n✅ Paged KV cache (block size 64)\\n⚡ Performance: 3000 GB/s memory-bound | BF16 580 TFLOPS compute-bound on H800\\n\\n## Day 2 - DeepEP\\n\\nExcited to introduce DeepEP - the first open-source EP communication library for MoE model training and inference.\\n\\n🔗 DeepEP GitHub Repo\\n✅ Efficient and optimized all-to-all communication\\n✅ Both intranode and internode support with NVLink and RDMA\\n✅ High-throughput kernels for training and inference prefilling\\n✅ Low-latency kernels for inference decoding\\n✅ Native FP8 dispatch support\\n✅ Flexible GPU resource control for computation-communication overlapping\\n\\n## Day 3 - DeepGEMM\\n\\nIntroducing DeepGEMM - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3/R1 training and inference.\\n\\n🔗 DeepGEMM GitHub Repo\\n⚡ Up to 1350+ FP8 TFLOPS on Hopper GPUs\\n✅ No heavy dependency, as clean as a tutorial\\n✅ Fully Just-In-Time compiled\\n✅ Core logic at ~300 lines - yet outperforms expert-tuned kernels across most matrix sizes\\n✅ Supports dense layout and two MoE layouts\\n\\n## Day 4 - Optimized Parallelism Strategies\\n\\n✅ DualPipe - a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.\\n🔗 GitHub Repo\\n\\n✅ EPLB - an expert-parallel load balancer for V3/R1.\\n🔗 GitHub Repo\\n\\n📊 Analyze computation-communication overlap in V3/R1.\\n🔗 GitHub Repo\\n\\n## Day 5 - 3FS, Thruster for All DeepSeek Data Access\\n\\nFire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.\\n\\n⚡ 6.6 TiB/s aggregate read throughput in a 180-node cluster\\n⚡ 3.66 TiB/min throughput on GraySort benchmark in a 25-node cluster\\n⚡ 40+ GiB/s peak throughput per client node for KVCache lookup\\n🧬 Disaggregated architecture with strong consistency semantics\\n✅ Training data preprocessing, dataset loading, checkpoint saving/reloading, embedding vector search & KVCache lookups for inference in V3/R1\\n\\n📥 3FS → https://github.com/deepseek-ai/3FS\\n⛲ Smallpond - data processing framework on 3FS → https://github.com/deepseek-ai/smallpond\\n\\n## Day 6 - One More Thing: DeepSeek-V3/R1 Inference System Overview\\n\\nOptimized throughput and latency via:\\n🔧 Cross-node EP-powered batch scaling\\n🔄 Computation-communication overlap\\n⚖️ Load balancing\\n\\nProduction data of V3/R1 online services:\\n⚡ 73.7k/14.8k input/output tokens per second per H800 node\\n🚀 Cost profit margin 545%\", '<source name=\"https://medium.com/@jjjy213/deepseek-v3-explained-fdac83ba280c\"/>\\nauthor - Ataka jeong\\n\\n1. Introduction\\nHow could the DeepSeek-V3 model achieve incredible performance and economical training as an open source model? In this paper review, we will explore the various features that were invented and applied to build the DeepSeek-V3 model. The way the paper presents the model may seem complicated to people who are unfamiliar with the new conceptualization invented by DeepSeek. However, its core principle still resembles that of the standard Transformer and well-known LLMs. It will be incredibly helpful to have general knowledge of previously released large language models like LLaMA. I will also add my own interpretation of the DeekSeek model in this story.\\nLet’s dive into the new features of model architecture step by step.\\n2. Model Architecture\\nFirst of all, we will investigate the core architecture of DeekSeek-V3 model. The DeekSeek-V3 model has inherited most parts of model from previous V2 model. These parts of model were elaborated more in V2 paper, but it is worth noting the principle how V3 model was built upon. While they used the structure of the ordinary transformer block like Llama, its attention and Feed-Forward Network were more sophisticated to boost the model performance. The overview of the Transformer block is as shown in the following diagram.\\nThe two main components are Multi-Head Latent Attention(MLA) and DeepSeekMoE.\\n- 2.1 Multi-Head Latent Attention(MLA)\\nWhat is Multi-Head Latent Attention(MLA)? You might noticed that “Latent” is only additional word to conventional attention module. MLA improved the speed and memory usage in the attention block by compressing the input vector. From a data analysis perspective, the data can be compressed into a lower dimension while preserving the information it contains. One of the well-known techniques is Principal component analysis (PCA), which reduces the dimension of the data and maintains variance to retain its information. In latent diffusion model, the input data is compressed by variational autoencoder and reconstructed in initial dimension. The Multi-Head Latent Attention(MLA) applied this principle to compress and decompress the input data. By storing a compressed vector for the KV cache, the DeepSeek model can improve both speed by reducing data copying and memory efficiency by using a smaller compressed vector. The weight matrix for compression is additionally required, because human cannot compress it manually, but AI should learn it how the compression should be done. Applying RoPE to the compressed vector is not mathematically compatible, which was shown in detail in V2 paper, they used decoupled RoPE. As illustrated in the figure above, RoPE is applied to query and key, but also the query and key without RoPE. The RoPE-applied query and key are then concatenated with their respective non-RoPE counterparts. Finally, the query and key are obtained as normal transformer block, then the computation of dot-product attention will not be different from original one. But we could reach this point with a more economical KV cache thanks to the lower dimension of data.\\n- 2.2 DeekSeekMoE\\nSecondly, you can note that Feed-Forward Network is unusual as it was split into a lot of experts, rather than one large FFN. They called it as DeekSeekMoE. Like humans in a group, the AI also needs to specialized in certain domain to improve the performance. Thus, the mixture of experts come into play here. Each expert can specialize in certain domain, in this case, the group of tokens that they are familiar with, instead of coping with entire range of tokens alone. Dependent on the input sequence(tokens), the certain experts are selected to be activated and they contribute to make output. Shared experts are generalist and are activated for all kind of tokens. Then it might be interesting to know by what algorithm we can select the experts? We need to assign a vector to each expert which determines the range of tokens(domain) that experts can deal with well. And we give score to each expert to check how similar the domain of expert and input token are. If the score is high, then we should select the expert and let them activated to make output. Well, it sounds quite simple. Let’s see the math behind it.\\neᵢ is a centroid vector. It is learned during training and represents the type of input tokens the expert specialized in. Each expert’s centroid vector encodes the knowledge domain it specializes in.\\nuₜ is input vector to FFN. The dot product uₜᵀ eᵢ quantifies the similarity between the input vector uₜ\\u200b and the centroid (or domain) of expert eᵢ, effectively measuring the alignment of the input data with the expert’s specialized domain. So, the sᵢ = Sigmoid(uₜᵀ eᵢ) represents the score for each i-th expert, determining whether the expert should be selected. By gating value gᵢ, which select Kᵣ experts with high score by Topk algorithm. We add all outputs of selected experts and shared experts, then we arrive to the final output.\\n2.3 Multi-Token Prediction\\nIn a standard transformer, the model generates a token each time, and this new token is fed back into decoder as input. Since this way restricts the efficiency and the speed of convergence during training, many researchers have made effort to come up with a method to generate multiple tokens each time. DeepSeek improved the conventional way of Multi-Token Prediction(MTP). Instead of previous parallel MTP, DeepSeek decided sequential MTP. They construct independent MTP modules, where the previous output of the Transformer block is concatenated into the subsequent MTP module, as illustrated in the following figure.\\nAs shown in the figure, the structure of MTP modules is akin to RNN model. But, unlike RNN, which preserve hidden states of nodes, the MTP modules send output of prediction to the subsequent module. Even though a single Transformer block cannot generate multiple tokens, the entire system of MTP modules collectively enables multi-token prediction. As it compares additional tokens per prediction, it provides more information for weight updates during training, leading to more efficient learning and faster convergence. The model can proactively learn and prepare for the additional tokens.\\nIn actual training, DeepSeek opted to generate only one additional token, presumably due to the computational cost which is caused by using many MTP modules. It necessitate the compromise between the benefits of MTP and computational cost. During inference, the MTP modules are discarded, generating only one token per prediction.\\n3. Infrastructure\\n3.1 DualPipe\\nSince the U.S. did not export great GPUs like the NVIDIA H100 to China, DeepSeek researchers had to devise innovative methods to accelerate model training using the weaker H800 GPUs. Since they succeeded, NVIDIA’s stock price briefly plunged, as people believed that high-performance GPUs would no longer be necessary for training LLM. Because the DeepSeek model was trained on 2048 H800 GPUs, communication between GPUs accounts for large portion of training time. Therefore, enhanching networking between GPUs has to play crucial role to reduce training time. When we use many GPUs simultaneously, the GPUs have to wait for a certain amount of time until new data is copied from other GPU. This waiting time, which causes training inefficiencies, is known as a “bubble,” and we should minimize it as much as possible. DeepSeek invented a innovative method to reduce bubble.\\nDuring model training, data flows through the model in forward and backward processes. In forward process, data goes from the input layer to the output layer. On the other hand, during the backward process data moves from the output layer to the input later, updating weights based on the information to minimize the loss. Prior to DeepSeek, researchers found that the backward process can be split into two processes, which are backward for input and backward for weight, in order to remove more bubbles. The backward for input is computation of the gradient of the loss with respect to the input data, whereas the backward for weight calculates gradient of the loss with respect to the weight. The backward for input must be completed ahead of the backward for weight, because it is necessary to compute the backward for weight. Mathematically, the chain rule is applied to the calculation of backpropagation, where the backward for input is used for the calculation of backward for weight.\\nIn such process, it is certain that an enormous number of communications between GPUs is required. In order to reduce the number of communication, the DeepSeek’s DualPipe combines the forward process and the backward for input by initiating training data from two devices in the opposite directions as illustrated in following figure.\\n\\nThe batch 0 is the initial data, which starts processing on the device 0 and continues on the subsequent devices. In a conventional training plan, the device 7 remains idle, waiting for the batch 0 to be copied onto it. However, DualPipe makes the device 7 start training with other batch data in the opposite direction. This allows us to combine them as a chunk and continuously copy them together on other devices to reduce communication between GPUs. With weaker H800 GPUs, they couldn’t improve the speed of the GPUs, but reduce the communication between GPUs to accelerate the training.\\n\\n3.2 Mixed precision training\\nMixed precision training is already prevalent technique of LLM to improve training and memory efficiency while maintaining the model accuracy. In mixed precision training, it is critical task to find out which parts of model are less significant for the model accuracy and reduce the precision that parts. In DeepSeek-V3 model, the researchers have found that they should reduce precision in the parts of model where heavy computations are executed, such as matrix multiplication. In contrast, they preserved high precision for matrix addition and storing data, which are relatively lightweight computation. The mixed precision training of DeepSeek is shown in the following figure.\\n\\nWhile reducing the precision by the method above, overflow and underflow arise as a impediment. If the numerical values are quantized in lower precision like FP8 format, the values are clipped to the certain representable range. While computation in lower precision, the values can easily exceed the range during the computation. Scaling the values can mitigate the overflow and underflow by adjusting the values and lead to more proper representation in limited range. But, static scaling, which applies fixed scaling factor to all values, can still cause overflow and underflow for many values. To cope with this issue, DeepSeek implemented Fine-Grained Quantization. In this method, the values are grouped, and each group has its own scaling factor. This approach allows the each group of values to have a more suitable scaling factor, by which the overflow and underflow can be averted.\\nAnother issue of quantization is that the small errors can be accumulated and become more serious problem later. In order to avoid that a lot of values with error are summed and their errors are accumulated, intermediate values are copied in high precision, if the number of values reaches the interval. It means that some values are grouped, and their values are stored in high precision. Then, the errors of values aren’t accumulated on a large scale, because the small group of values don’t contribute to large error.\\nThese two techniques to prevent quantization error are visualized in following figure.\\n\\n4. Reinforcement Learning\\nAfter supervised fine-tuning, DeepSeek additionally implemented reinforcement learning. A reward model has to be built and trained for reinforcement learning, which gives feedback to the model and determine the direction of learning. The rule-based reward model(RM) and model-based reward model(RM) were employed.\\nThe rule-based RM is applied to the questions with specific rules, such as math problems and LeetCode problems. In these domains, the specific rules are used to verify the correctness of the answers and the questions about logical reasoning are involved. However, for many questions, the answer cannot be verified by a specific rule. In those cases where no rule is provided, the model-based RM determines, whether the answer matches the ground-truth answer. Another innovative idea of DeepSeek is including the chain-of-thought to the reward, whereas conventional models only included final reward based on the answer.\\nDeepSeek-V3 model, as V2 model did, adopted Group Relative POlicy Optimization (GRPO). This GRPO algorithm maximizes the following objective by updating the policy model π.\\n\\nMaximize this objective by updating the weights of the model based on the reward.\\n\\nAdvantage is defined as the normalized reward.\\nIn LLM case, the policy model π is model itself, and θ is weights of the model. q is question and o is output of the model. We can interpret the policy model(LLM) outputs a probability distribution over tokens, where the policy π(o|q) is a probability of output o given the question q. Therefore, the policy model is LLM itself. If the output o is right answer, we should reinforce the probability of that model makes this output o. So we need to maximize π(o|q) by multiplying advantage(normalized reward). If the output o is correct, the advantage (reward) will be a positive value and the policy will be reinforced. Otherwise, it will be negative and π(o|q) should be minimized. Plus, we have a fine-tuned model as the initial base model and do not want it to go too far from this base model, which might cause model to forget basic language understanding and important knowledge that the model learned during pre-training and fine-tuning. To implement this safety concerns, GRPO algorithm used KL divergence and epsilon parameter. The KL divergence measures the difference between current policy model and reference policy model(initial base model). So the KL divergence term should be minimized to maximized the GRPO objective. And we pick minimum between the original policy and the clipped policy in (1-ε, 1+ε), by which the model cannot deviate too much from 1. So, the current policy cannot differ a lot from the old policy, restricting the effect of reinforcement learning. This GRPO algorithm based on rule-based and model-based reward model enhances model performance and reasoning capability.\\n\\n5. Conclusion\\nDeepSeek-V3 model offered great opportunity for efficient training with cheaper GPUs. It is unclear that its performance exceeds the OpenAI model, but DeepSeek is way more economical to train and open-source model. AI researchers can directly use DeekSeek models and they can also implement the innovative ideas and designs in their own model, because the new methods of DeepSeek and source code are opened. Seemingly, the DeepSeek researchers have potential to come up with more advanced idea to improve the model performance and efficient training process. In AI development, a lower training cost almost always implies better model accuracy later on, as the data and model can easily be scaled up at a lower cost. I hope that the performance of a good AI model does not have to be undermined by the censorship and suppression of the Chinese government.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Path to the PDF file\n",
        "pdf_file_path = os.path.join(md_folder, '2501.12948v1.pdf')\n",
        "\n",
        "# Initialize the PDF reader\n",
        "reader = PdfReader(pdf_file_path)\n",
        "\n",
        "# Extract text from each page\n",
        "pdf_text = \"\"\n",
        "for page in reader.pages:\n",
        "    pdf_text += page.extract_text()\n",
        "\n",
        "# Print a preview of the extracted text\n",
        "print(pdf_text[:1000])  # Print the first 1000 characters of the text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9fqoNrolNY9",
        "outputId": "408ab283-7396-46f7-b601-1c487f0fb469"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
            "Reinforcement Learning\n",
            "DeepSeek-AI\n",
            "research@deepseek.com\n",
            "Abstract\n",
            "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
            "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\n",
            "vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\n",
            "Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\n",
            "reasoning behaviors. However, it encounters challenges such as poor readability, and language\n",
            "mixing. To address these issues and further enhance reasoning performance, we introduce\n",
            "DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\n",
            "R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\n",
            "research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n",
            "(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fitz tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I87iSj2JlQDe",
        "outputId": "36e3a30a-1fd9-4be2-c674-e966c240904a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fitz in /usr/local/lib/python3.11/dist-packages (0.0.1.dev2)\n",
            "Collecting tools\n",
            "  Downloading tools-0.1.9.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.11/dist-packages (from fitz) (5.0.9)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.11/dist-packages (from fitz) (7.2.0)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.11/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from fitz) (5.3.2)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.11/dist-packages (from fitz) (1.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fitz) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fitz) (2.2.2)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.11/dist-packages (from fitz) (1.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from fitz) (1.13.1)\n",
            "Collecting pytils (from tools)\n",
            "  Downloading pytils-0.4.1.tar.gz (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tools) (1.17.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from tools) (5.3.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2->fitz) (3.2.1)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->fitz) (6.5.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel->fitz) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel->fitz) (4.12.2)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (8.1.8)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.4.2)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (2.0.1)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (6.3.2)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.20.1)\n",
            "Requirement already satisfied: traits>=6.2 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (7.0.2)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.17.0)\n",
            "Requirement already satisfied: acres in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: etelemetry>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (0.3.1)\n",
            "Requirement already satisfied: looseversion!=1.2 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: puremagic in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (1.28)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->fitz) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fitz) (2025.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from pyxnat->fitz) (2.32.3)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.11/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.11/dist-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->pyxnat->fitz) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->pyxnat->fitz) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->pyxnat->fitz) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->pyxnat->fitz) (2025.1.31)\n",
            "Building wheels for collected packages: tools, pytils\n",
            "  Building wheel for tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tools: filename=tools-0.1.9-py3-none-any.whl size=46729 sha256=55dfcf2676ea11db44b8522d9f09b716da07606da04ef58cf2f82ff9a9e13852\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/d8/9d/52ad6058db295741fe0b776c0fcfdb6670036acab59ce4ccfd\n",
            "  Building wheel for pytils (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytils: filename=pytils-0.4.1-py3-none-any.whl size=32876 sha256=6c91f09b6ce2ea8a350a6bd337add264284f1bc078b0a5d97e25cf93de985475\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/b9/15/482258065bee884b0d43bdc24b424e2cc6bde530e8e1380657\n",
            "Successfully built tools pytils\n",
            "Installing collected packages: pytils, tools\n",
            "Successfully installed pytils-0.4.1 tools-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pymupdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E4U8_j9vJxb",
        "outputId": "d9a246fa-6772-4b6d-bdfa-fc027e7ced6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ5u2J9dvnOr",
        "outputId": "5d0d7fc2-0b1e-4de7-e008-bc1c5d9a852a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Combine all text content (Markdown and PDF) into a single list\n",
        "combined_text = [pdf_text] + md_text\n",
        "\n",
        "# Create a dataset object\n",
        "dataset = Dataset.from_dict({\"text\": combined_text})\n",
        "\n",
        "# Split the dataset into training and testing (80-20 split)\n",
        "train_test_split = dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "test_dataset = train_test_split[\"test\"]\n",
        "\n",
        "# Preview the datasets\n",
        "print(train_dataset[:2])  # Preview the first two items of the training dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM8Gv4CEv_iC",
        "outputId": "93a57986-6dd7-42fd-ce40-7cfa68c366e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['<source name=\"https://medium.com/@visithkumarapperuma/deepseek-v3-a-game-changer-in-a-i-heres-why-it-matters-75591957ca07\">\\n\\nauthor - Visith Kumarapperuma\\n\\n# Deepseek V3: A Game-Changer in A.I. Here’s Why It Matters\\n\\nCurrently, the AI models from the Chinese startup Deepseek are causing quite a stir in the AI space. Their latest reasoning model, Deepseek r1, shows better or equal performance to competitors. But above all, they achieved it with a fraction of the training and inference cost.\\nDeepSeek’s AI Assistant overtook ChatGPT to become the most downloaded free app on the U.S. App Store. This development has led to market concerns about A.I. investments to major U.S. tech companies. Impacting share prices of tech firms including Nvidia.\\n\\n## So what made Deepseek such a big impact to A.I. ?\\nThe significance of Deepseek as a disruptor in the industry lies in its approach. Unlike other companies that pushed for better hardware, Deepseek improved the algorithms. Thus achieving better results at a software level.\\nNote that the following details are for the Deepseek V3 model.\\n• Deepseek said it trained a model using a data centre of some 2,000 of Nvidia H800 GPUs.\\n• Time duration 2 months with the cost of the *final training run being ~$5.5 million\\nThis ~$5.5M reflects the “rental” cost for the GPU hours needed to train DeepSeek‑V3. It does not include:\\n1. The capital expenditure for owning the hardware.\\n2. Costs associated with prior research, ablation studies, or experiments on alternative architectures/algorithms/data.\\n\\n### Deepseek made training more efficient (45 times more efficient)\\n- Use 8-bit instead of 32-bit to save memory.\\n- Compress key value indices which eat up a lot of VRAM; they got 93% compression ratios.\\n- Do multi-token prediction instead of single-token prediction -> doubled inference speeds\\n- The MOE model decomposes a big model into small models that can run on consumer-grade hardware.\\n\\n## Summary of how Deepseek v3 was so efficient at training the frontier model\\n1. Model Architecture\\nThe model employs a Mixture-of-Experts (MoE) architecture, where only 37B parameters fire for each token out of the total 671B. This sparse activation significantly reduces compute requirements compared to dense models.\\nThe model uses Multi-head Latent Attention (MLA). This compresses the Key-Value cache, reducing memory usage and enabling more efficient training.\\n2. FP8 Mixed Precision Training:\\nThey implemented an FP8 mixed precision training framework. Which reduces memory usage and accelerates training compared to higher precision formats.\\nReduced memory footprint by up to 50% compared to traditional FP16/FP32 formats.\\nThey use fine-grained quantisation strategies and increased accumulation precision to maintain accuracy.\\n3. Load Balancing Strategy\\nThey pioneered an auxiliary loss-free strategy for load balancing in the MoE architecture. This improved performance without the drawbacks of traditional auxiliary loss methods.\\n4. Training Framework\\nThey developed a custom training framework called HAI-LLM with several optimisations:\\nDualPipe algorithm for efficient pipeline parallelism. This reduces pipeline bubbles and overlapping computation and communication.\\nEfficient cross-node all-to-all communication kernels to fully utilise network bandwidth.\\nCareful memory optimisations to avoid using costly tensor parallelism.\\n\\n## Breakdown of the costs of the Deepseek v3 model\\nDeepseek’s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token\\n- Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework.\\n- Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet.\\n- For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead.\\n- Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million\\n- the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens.\\n`So how true is the claim of $5.5 million, or is it another marketing trick?`\\n\\n1. Underlying FLOP calculations\\nModel Details:\\n- Active Parameters: 37B (using FP8 precision)\\n- FLOPs per token: Using the rule of thumb “6 FLOPs per parameter per token.”\\n`37B×6 = 222B FLOPs per token`\\n- Total Training Tokens: Approximately 14.8 trillion tokens\\n- Total FLOPs required:\\n`222 B FLOPs/token×14.8 T tokens ≈ 3.3×10²⁴ FLOPs`\\n### GPU FLOP Capacity (H800/H100):\\nAn H100 is roughly estimated to deliver about.\\n3.958×10¹⁵ FLOPs (per second or per some standardised interval — here used as a comparative metric).\\nIdeal (Perfect Efficiency) GPU hours.\\n(Dividing total required FLOPs by per‑GPU capability gives)\\n`3.3×10²⁴ / 3.958×10¹⁵ \\u200b≈ 8.33×10⁸ seconds⇒≈0.4 million GPU hour`\\nNote: This “perfect efficiency” scenario is a lower bound. Real-world training is less efficient.\\n2. Adjusting for Real‑World Inefficiencies (Comparison with Llama 3.1)\\nReference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice.\\nRecalculating FLOPs for Llama 3.1:\\n`Using the same math: 3.64×10²⁵ FLOPs required`\\nScaling Efficiency\\nUsing the ratio of FLOPs needed for DeepSeek‑V3 versus Llama 3.1. and assuming similar inefficiencies.\\nThe estimate adjusts to roughly 2.79M GPU hours for DeepSeek‑V3 training.\\n3. DeepSeek‑V3 Reported Training Breakdown\\nAccording to the DeepSeek‑V3 paper\\nPre‑training Stage:\\n- Per Trillion Tokens: 180K H800 GPU hours\\n- Overall Pre‑training: Total of 2,664K GPU hours\\n- This stage was completed in less than two months using a cluster of 2,048 H800 GPUs.\\nContext Length Extension:\\n- Additional 119K GPU hours\\nPost‑training:\\n- An extra 5K GPU hours\\nTotal GPU Hours:\\n`2,664 K+119 K+5 K≈2.788M GPU hours`\\n4. Cost Estimation\\nAssumed GPU Rental Price: $2 per GPU hour\\nTotal Rental Cost:\\n`2.788M GPU hours×$2/hour≈$5.576 million`\\nas stated in Deepseek paper\\nDuring the pre‑training stage, training DeepSeek‑V3 on each trillion tokens requires only 180K H800 GPU hours… Consequently, our pre‑training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post‑training, DeepSeek‑V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M.\\n5. Summary\\nTheoretical (Perfect Efficiency) Estimate: ~0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0\\nAdjusted (Real‑World) Estimate (via Llama 3.1 comparison): ~2.79 GPU hours\\nDeepSeek‑V3 Reported Breakdown:\\nPre‑training: 2,664K GPU hours\\nContext Extension: 119K GPU hours\\nPost‑training: 5K GPU hours\\nTotal: ~2.788 M GPU hours\\n### Cost (at $2 per GPU hour): ~$5.576 million', '# DualPipe\\nDualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.\\n\\nPipeline Bubbles and Memory Usage Comparison\\n\\n| Method    | Bubble                  | Parameter | Activation |\\n|:---------:|:-----------------------:|:---------:|:----------:|\\n| 1F1B      | (PP-1)(𝐹+𝐵)            | 1×        | PP         |\\n| ZB1P      | (PP-1)(𝐹+𝐵-2𝑊)         | 1×        | PP         |\\n| DualPipe  | (PP/2-1)(𝐹&𝐵+𝐵-3𝑊)     | 2×        | PP+1       |\\n\\n𝐹 denotes the execution time of a forward chunk, 𝐵 denotes the execution time of a full backward chunk, 𝑊 denotes the execution time of a \"backward for weights\" chunk, and 𝐹&𝐵 denotes the execution time of two mutually overlapped forward and backward chunks.\\n\\n### About\\nA bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training\\n\\n`DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.`\\n\\n# Profiling Data in DeepSeek Infra\\nHere, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details. The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser). Notice that we simulate an absolutely balanced MoE routing strategy for profiling.\\n\\n## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)\\n\\nWhen using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\\n\\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo\\'s scope. A common method is to use moving average of historical statistics.\\n\\n## The Algorithm\\n\\nThe load balancing algorithm comes with two policies used for different cases.\\n\\n## Hierarchical Load Balancing\\n\\nWhen the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\\n\\n### Global Load Balancing\\n\\nIn other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.\\n\\n# Fire-Flyer File system\\nThe Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\\n\\n- Performance and Usability\\n\\n    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\\n    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\\n    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.\\n\\n- Diverse Workloads\\n\\n    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\\n    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\\n    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\\n    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\\n\\n## Performance\\n1. Peak throughput\\n\\nThe following figure demonstrates the throughput of read stress test on a large 3FS cluster. This cluster consists of 180 storage nodes, each equipped with 2×200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC. The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs.\\n\\n2. GraySort\\n\\nWe evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets. Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.\\n\\nThe test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2×400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1×200 Gbps NIC/node). Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min.\\n\\n3. KVCache\\n\\nKVCache is a technique used to optimize the LLM inference process. It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers. The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s. The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load Qwen2.5-3B model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-3B\"  # Use the specific model name from HuggingFace if needed\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Check model and tokenizer\n",
        "print(model)\n",
        "print(tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "ddbe7fbbf7f74175bb9ee919b037232c",
            "338850c32a0a4967be7ad7955055095f",
            "5ba3b5355ad84e79b1a48fcc882183d7",
            "fa2e6d629c884ec4ba29ef251b7720ef",
            "bd570277e39945839e94f3f590c0c230",
            "906877a68b5c44b4b115c23e576710d7",
            "91a06d92f9f948b39cd8973c04360bf1",
            "72906650a4804ad495ac017aa3c1c131",
            "7917f1373ad14a91b2766123faf7dcf1",
            "0c4e7c2b0a7a41ffbd121cd13c0d8f00",
            "1f9adb636f9b423d929561f44605fe30"
          ]
        },
        "id": "8-bhKMF13mxF",
        "outputId": "c7273568-b01e-4523-9c50-dfdba876b89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddbe7fbbf7f74175bb9ee919b037232c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Example dataset (replace with actual data)\n",
        "dataset = Dataset.from_dict({\"text\": combined_text})  # 'combined_text' contains your dataset\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Check tokenized dataset\n",
        "print(tokenized_datasets[\"train\"][:2])  # Preview the first 2 entries\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "93c64deaccbe4386a67dcacafb20bd29",
            "791ee22b8499434b953bb958ac4c989f",
            "db44f5e74eee4d47898abe35cc5eb3bb",
            "70f41ef64c6a49afa3c7ead6269cd7b4",
            "387e189b86b1407196c6a128e041c568",
            "8851c2da2c7d415db097343985ced603",
            "6d47044f8d9f46dca3c2759588eca06e",
            "921cf196b7d6489cbed9447ee1afd773",
            "5cbe58ffbfc945e2bae11cd3df9354d8",
            "9fa6f7d1e1724d45bcdf5eb8cf77e51c",
            "7f652f862e2c481e80c6b38a97ab17b7"
          ]
        },
        "id": "Vc5rZqQdweWp",
        "outputId": "9d12f81a-ab4c-435c-b350-bbdb258e0333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93c64deaccbe4386a67dcacafb20bd29"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n"
      ],
      "metadata": {
        "id": "lpzgnMU0wjYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ddb6682-815b-41fd-dbb4-b471ca8244b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "liyUr6TTRLpk",
        "outputId": "53075c67-a6bd-4909-b7b9-175dbd6bc651"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4acf8ed36fd0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize the Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m trainer = Trainer(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./fine_tuned_Qwen2.5-3B\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_Qwen2.5-3B\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "3gh1faBLROXm",
        "outputId": "629301ed-71fa-4145-aa00-4022a7d9202d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a154f15a389b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the fine-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./fine_tuned_Qwen2.5-3B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./fine_tuned_Qwen2.5-3B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the fine-tuned model for inference\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_Qwen2.5-3B\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_Qwen2.5-3B\")\n",
        "\n",
        "# Set up inference pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Generate a response\n",
        "input_text = \"What are the advancements in AI research for 2025?\"\n",
        "output = generator(input_text, max_length=200)\n",
        "\n",
        "# Print the output\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "PirJJbsrRT02",
        "outputId": "d5984023-31aa-493e-c9c8-6d1b1be24adb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c0c60f3d850f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the fine-tuned model for inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./fine_tuned_Qwen2.5-3B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./fine_tuned_Qwen2.5-3B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2kvqHv2rRW2v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}